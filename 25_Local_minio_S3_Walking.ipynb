{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MDF Analysis from S3 w/local caching.\n",
    "\n",
    "Example Scenario : \n",
    "\n",
    "- MDF data, sitting on a local hard drive. eg, CANape recordings from a vehicle at any point in the development cycle.\n",
    "\n",
    "### ```minio``` Setup.\n",
    "\n",
    ">  MinIO is a cloud storage server compatible with Amazon S3, released under Apache License v2.\n",
    "\n",
    "> As an object store, MinIO can store unstructured data such as photos, videos, log files, backups and container images. The maximum size of an object is 5TB. \n",
    "\n",
    "- Install the [minio binary for your machine](https://docs.min.io/docs/minio-quickstart-guide.html).\n",
    "- Start a Minio instance pointing at your datafiles.  \n",
    "  ```minio.exe serve C:\\CANapeRecordings```\n",
    "  \n",
    "### Pony ORM\n",
    "\n",
    "> Pony is a Python ORM with beautiful query syntax  \n",
    "> Write your database queries using Python generators & lambdas  \n",
    "\n",
    "> - Supports major relational databases: SQLite, PostgreSQL, MySQL, Oracle \n",
    "\n",
    "https://ponyorm.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "from asammdf import MDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ```minio``` configs.\n",
    "\n",
    "The many ways to use a custom S3 endpoint url:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example a large dataset of fake information is generated with asammdf and scipy signal generators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the data through the native file system and again through ```fsspec``` and minio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -skh /projects/MDF_Data_Pipeline/Data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -skh /projects/MDF_Data_Pipeline/Data/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.ls(\"AerospaceStartup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk Through All Files:\n",
    "\n",
    "Walk through all S3 files and find the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "import os\n",
    "mdf_paths=list()\n",
    "for company in fs.ls(\"\"):\n",
    "    for root, dirs, files in fs.walk(company):\n",
    "        for file in files:\n",
    "            if file.lower().endswith(\".mf4\") or file.lower().endswith(\".mdf\"):\n",
    "                mdf_paths.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mdf_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop through all mdf files found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mdf_path in mdf_paths:\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print off the information ```fsspec.info```  contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.info(mdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal 1: Find the largest MDF file indexed through minio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all the MDF file sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sizes = [fs.info(mdf_path)[\"size\"] for mdf_path in mdf_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/6423325\n",
    "myList = [1, 2, 3, 100, 5]    \n",
    "sorted(range(len(myList)),key=myList.__getitem__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_idx = sorted(range(len(sizes)),key=sizes.__getitem__)[-1]\n",
    "biggest_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf_paths[biggest_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fs.info(mdf_paths[biggest_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_mdf = fs.info(mdf_paths[biggest_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_mdf[\"size\"]/1024**3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MB:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biggest_mdf[\"size\"]/1024**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/dask/s3fs/issues/273\n",
    "# https://github.com/pandas-dev/pandas/pull/29050\n",
    "%env AWS_ACCESS_KEY_ID=\"minioadmin\"\n",
    "%env AWS_SECRET_ACCESS_KEY=\"minioadmin\"\n",
    "%env S3_ENDPOINT=\"http://127.0.0.1:9000\"\n",
    "\n",
    "fs2=s3fs.S3FileSystem(\n",
    "    client_kwargs={\n",
    "        \"endpoint_url\": \"http://127.0.0.1:9000\",\n",
    "    })\n",
    "fs2.ls(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asammdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "of = fs.open(mdf_paths[biggest_idx])\n",
    "mdf = asammdf.MDF(of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "of = fs.open(mdf_paths[biggest_idx])\n",
    "\n",
    "mdf = asammdf.MDF(of)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdf.channels_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mdf.get_group(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to fsspec filecache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exact example from anaconda article: [Introducing Remote Content Caching with FSSpec.](https://www.anaconda.com/fsspec-remote-caching/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fsspec\n",
    "of = fsspec.open(\"filecache://anaconda-public-datasets/iris/iris.csv\", mode='rt', \n",
    "                 cache_storage='/tmp/cache1',\n",
    "                 target_protocol='s3', target_options={'anon': True})\n",
    "with of as f:\n",
    "    print(f.readline())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = \"/tmp/mdf_cache\"\n",
    "fsspec_kwargs = {\n",
    "    \"urlpath\": f\"filecache://{mdf_path}\",\n",
    "    \"mode\": 'rb', \n",
    "    \"cache_storage\": cache_dir,\n",
    "    \"target_protocol\": 's3',\n",
    "    \"target_options\": s3_cfg,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree(\n",
    "    path=cache_dir,\n",
    "    ignore_errors=True\n",
    ")\n",
    "t1=time.time()\n",
    "with fsspec.open(**fsspec_kwargs) as of:\n",
    "    mdf = MDF(of)\n",
    "t2=time.time()\n",
    "with fsspec.open(**fsspec_kwargs) as of:\n",
    "    mdf2 = MDF(of)\n",
    "t3=time.time()\n",
    "\n",
    "print(f\"Uncached Read: {t2-t1}s\")\n",
    "print(f\"Cached Read: {t3-t2}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All S3 Files Cached/Uncached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "mdf_paths=list()\n",
    "for root, dirs, files in fs.walk(\"canedge-live-demo-2\"):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".mf4\"):\n",
    "            mdf_paths.append(os.path.join(root, file))\n",
    "len(mdf_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdfs_uncached = list()\n",
    "mdfs_cached = list()\n",
    "\n",
    "cache_dir = \"/tmp/mdf_cache\"\n",
    "\n",
    "\n",
    "\n",
    "shutil.rmtree(\n",
    "    path=cache_dir,\n",
    "    ignore_errors=True\n",
    ")\n",
    "t1=time.time()\n",
    "for mdf_path in mdf_paths:\n",
    "    fsspec_kwargs = {\n",
    "        \"urlpath\": f\"filecache://{mdf_path}\",\n",
    "        \"mode\": 'rb', \n",
    "        \"cache_storage\": cache_dir,\n",
    "        \"target_protocol\": 's3',\n",
    "        \"target_options\": s3_cfg,\n",
    "    }\n",
    "    with fsspec.open(**fsspec_kwargs) as of:\n",
    "        mdfs_uncached.append(MDF(of))  \n",
    "        \n",
    "t2=time.time()\n",
    "\n",
    "for mdf_path in mdf_paths:\n",
    "    fsspec_kwargs = {\n",
    "        \"urlpath\": f\"filecache://{mdf_path}\",\n",
    "        \"mode\": 'rb', \n",
    "        \"cache_storage\": cache_dir,\n",
    "        \"target_protocol\": 's3',\n",
    "        \"target_options\": s3_cfg,\n",
    "    }\n",
    "    with fsspec.open(**fsspec_kwargs) as of:\n",
    "        mdfs_cached.append(MDF(of))  \n",
    "t3=time.time()\n",
    "\n",
    "print(f\"Uncached Read: {t2-t1}s\")\n",
    "print(f\"Cached Read: {t3-t2}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
